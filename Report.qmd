---
title: "Project Report"
author:
  - name: "Kristina Golden"
  - name: "Christopher Hill"
format: 
  html:
    toc: true    
    toc-depth: 3 
editor: visual
execute:
  echo: false
  message: false
  warning: false    
editor_options: 
  chunk_output_type: console
---

\newpage

```{r}
library(tidyverse)
library(caret)
library(lattice)
library(MLmetrics)
library(knitr)

train_data <- readRDS("training_data.rds")
test_data <- readRDS("testing_data.rds")
models <- readRDS("models.rds")
```

```{r}
test_metrics <- lapply(seq_along(models), function(i) {
  model <- models[[i]]
  model_name <- names(models)[i]
  
  probs <- predict(model, newdata = test_data, type = "prob")
  preds <- predict(model, newdata = test_data)

  cm <- confusionMatrix(preds, test_data$SUSPECT_ARRESTED_FLAG)

  accuracy <- cm$overall["Accuracy"]
  kappa <- cm$overall["Kappa"]
  
  positive_class <- levels(test_data$SUSPECT_ARRESTED_FLAG)[2]
  
  log_loss <- LogLoss(
    y_pred = as.matrix(probs), 
    y_true = as.numeric(test_data$SUSPECT_ARRESTED_FLAG == positive_class)
  )
  
  auc <- AUC(
    y_pred = probs[, positive_class], 
    y_true = as.numeric(test_data$SUSPECT_ARRESTED_FLAG == positive_class)
  )

  data.frame(
    Model = model_name,
    Accuracy = accuracy,
    Kappa = kappa,
    LogLoss = log_loss,
    AUC = auc
  )
})
```


# Introduction

## Problem Description

## Study Design and Sampling Methods

## Scientific Questions to Investigate

## Overview of Conclusions

### Chosen Model

### Key Findings

## Roadmap to Follow

The [Methods](#methods) section describes the statistical models used, the steps of data preprocessing, and the procedures for hyperparameter tuning. Additionally, visual and numeric summaries of the data are discussed to set the context for the analysis. The modeling methodology is presented in detail, with a comparison of multiple models and their performance criteria.

Here, we also address the scientific questions identified in the Introduction. The results from the models are presented and interpreted, highlighting key findings and their implications.

The [Conclusions](#conclusions) section summarizes the study objectives, the findings from the analysis, and how they relate to the overall project goals. Additional observations and limitations of the analysis are also discussed.

The [Appendix](#appendix) houses tables, source code, and supplementary plots for reference.

## Data Source Reference

[NYPD Stop, Question and Frisk Data](https://www.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page)

\newpage

# Methods {#methods}

## 1. Before Modeling

### Data Preprocessing Steps

The data was cleaned, transformed, and structured to suit the requirements of the statistical machine learning algorithms for our predictive models. The following steps were taken:

1. Data from multiple years (2018–2023) was read and combined into a single dataset. Column names were standardized by converting them to uppercase and replacing spaces with underscores for consistency. An additional column was added to indicate the year each record belonged to.

2. Strings such as "(null)" were identified as placeholders for missing data and replaced with proper `NA` values to facilitate further processing.

3. Several columns, particularly those representing numeric data like suspect height, weight, and stop durations, were converted from character strings to numeric.

4. Time and Date columns were converted to the appropriate data types for analysis.

5. Binary flag variables, containing values such as `Y`, `N`, `NA`, and occasionally invalid entries like `(`, were handled with special consideration to ensure usability in modeling. This process involved the following rules:
   a. *Columns with Only Y and`NA`Values*: If a column contained only `Y` and NA, the`NA`values were imputed with N. This assumption was made based on the interpretation that the absence of a `Y` flag implied the event did not occur.
   b. *Columns with `Y`, `NA`, and `(` Values*: First, all `NA` values were replaced with `N`. Then, invalid entries represented by `(` were converted to `NA`. This ensured no invalid characters remained in the dataset.
   c. *Columns with `Y`, `N`, `NA`, and `(` Values*: Invalid entries (`(`) were replaced with `NA`. Existing `Y`, `N`, and `NA` values were left unchanged, maintaining the integrity of the valid binary flags.
   d. *Other Scenarios*: For Flag columns not fitting the above patterns, the values were left as-is, as they did not conform to predefined rules or patterns for binary flags.

6. The extent of missingness across columns was evaluated. Variables with excessive missing data (more than 10,000 missing values) were excluded from further consideration. Remaining rows with missing values were removed to create a complete dataset.

7. Remaining character variables were converted into factors to enable categorical analyses. Some factors, like SUSPECT_RACE_DESCRIPTION, were consolidated by merging specific levels (e.g., "Black Hispanic" and "White Hispanic" were grouped under "Hispanic").

8. New predictors were created by aggregating related binary flag variables:
   a. *Physical Force Count*: Summed all physical force-related flags to capture the intensity of force used.
   b. *Suspect Actions Count*: Consolidated actions taken by suspects into a single predictor.
   c. *Weapon Flag Count*: Combined multiple weapon-related flags into one variable representing the presence of any weapon.
 
 9. All numeric columns were standardized by centering and scaling. This ensured that variables with different units or scales were treated equitably by the models.
 
 10. Variables deemed irrelevant or redundant, such as geographic coordinates (STOP_LOCATION_X, STOP_LOCATION_Y) were excluded. The final dataset was saved for use in modeling.

### Data Summaries

*Note: The summary statistics below are based on the training data only. Numeric vairables have been standardized.*

```{r}
summarize_data <- function(data) {
  # Handle numeric variables
  numeric_summary <- data |>
    select(where(is.numeric)) |>
    summarise(across(everything(), ~ {
      stats <- summary(.)
      stats <- round(stats, 2)
      paste(names(stats), stats, 
            sep = "\t", collapse = "<br>") 
    })) |>
    pivot_longer(cols = everything(), 
                 names_to = "Variable", 
                 values_to = "Summary")
  
  # Handle factor variables
  factor_summary <- data |>
    select(where(is.factor)) |>
    summarise(across(everything(), ~ {
      tbl <- table(.)
      prop <- prop.table(tbl)
      # Combine counts and proportions
      paste(names(tbl), tbl, 
            sprintf("(%.2f%%)", 
                    prop * 100), 
            sep = ":  ", collapse = "<br>")
    })) |>
    pivot_longer(cols = everything(), 
                 names_to = "Variable", 
                 values_to = "Summary")
  
  # Combine and display
  bind_rows(numeric_summary, 
            factor_summary) |>
    select(Variable, Summary) |>
    kable(format = "html", 
          col.names = c("Variable", "Summary"), 
          escape = FALSE)
}

summarize_data(train_data)
```



## 2. Statistical Models and Methods

We follow the guidelines for statistical modeling laid out in the Model Building and Data Splitting lecture notes.

1. *Split the data into a training set and a test set.*

2. *Tune hyperparameters (of all the models under consideration) using the training set:*
   a. *Split the training set further into two sets: one for fitting the model (a new training set), and the other for evaluating model performance (known as validation set or holdout set).*
   b. *For each candidate value of hyperparameter(s), fit the model using the new training set, and evaluate the fitted model using the validation set using a metric of our choice.*
   c. *Typically, we repeat steps a. and b. a few times so that we get repeated measurements of model performance for each value of hyperparameters. The final model performance is taken to be the average of these multiple measurements.*
   d. *Choose the best value of hyperparameters by optimizing the model performance measure obtained in step c.*

3. *Using the best value of hyperparameters, fit the model(s) on the entire training set and estimate the model parameters. This is (are) the final model(s) chosen using the training set.*

4. *Use the test set to estimate the model performance of the final model(s) from step 3.*

5. *Again, we may want to repeat steps 1–4 a few times to get a reliable estimate of model performance of the final models. For example, we can use cross-validation here to incorporate the uncertainty due to hyperparameter tuning as well.*

By leveraging the `caret` package in R, we can efficiently implement the above steps for a variety of models. The `train()` function in `caret` will handle steps 2 and 3 for us. This can be termed the "inner loop". After tuning hyperparameters and fitting the final trained model, we can then evaluate the various models on the test set. This can be termed the "outer loop". 

To evaluate the models in the outer loop and the very first step when modeling began, we used a 70/30 split, where 70% of the data was used for training and 30% for testing. This is known as the holdout method. For training in the inner loop, 5-fold cross validation was used on the training data split. The test data was never used in the inner loop, only for evaluating the final model performance.

### Model Descriptions and Assumptions

- **Random Forest** - Improves prediction accuracy by combining multiple decision trees. Each tree is trained on a random bootstrap sample of the data, and at every split, only a random subset of predictors is considered. This medigates the correlation between trees when a certain predictor may be present in all trees, reducing their variance and improving the robustness of the predictions.

- **Extreme Gradient Boosting (XGBoost)** - Builds models sequentially, each new tree correcting errors made by previous ones, and optimizes a loss function. XGBoost also uses regularization, meaning it penalizes complex models to prevent overfitting.

- **Logistic Regression** - Models the relationship between a set of predictor variables and a binary outcome by estimating the probability of the outcome occurring, using the logistic function to ensure probabilities fall between 0 and 1. The method identifies linear relationships between predictors and the log-odds of the outcome

- **Elastic Net** - A regularized regression method that combines the penalties of Lasso and Ridge regression. It balances variable selection and shrinkage by minimizing prediction error while preventing overfitting.

### Model Comparisons

#### Variables of Importance

[Variables of Importance](#variable-importance-plots)

#### Tuning Parameters

[Hyperparameter Tuning](#tuning-parameter-plots)

#### Trained Model Summaries

[Trained Model Summaries](#trained-model-summaries)

### Scientific Questions and Results Interpretation

## Performance Investigation and Criteria

We evaluate several metrics to compare the performance of the models as they generilize to our test set. These metrics include: AUC, LogLoss, Kappa, and Accuracy. 

[Test Metrics Results](#test-metrics)

### Metrics Used for Model Evaluation

- **AUC** (Area Under the *Receiver Operating Characteristic* Curve) - was used as our training metric of choice during model tuning. AUC measures how correctly a model distinguishes between true outcomes and false outcomes across all possible boundaries between classes. A model with a larger AUC value can be said to better distinguish between true and false outcomes across all boundaries between classes.

- **LogLoss** - quantifies how uncertain a model is about its predictions, penalizing incorrect predictions with greater uncertainty. A model with a smaller LogLoss value can be said to make more confident and accurate predictions because it assigns higher probabilities to correct outcomes and lower probabilities to incorrect ones, minimizing uncertainty.

- **Kappa** - measures how correct a model's predictions are compared to actual outcomes, while accounting for correctness expected by random chance. A model with a larger Kappa value can be said to make predictions that are more correct relative to chance alone.

- **Accuracy** - is the proportion of correctly predicted outcomes out of all predictions made. Included for its simplicity and interpretability.

## Limitations of Methods

\newpage

# Conclusions {#conclusions}

## Study Objectives Summary

## Findings and Interpretation

## Project Goals and Outcomes

## Additional Observations

\newpage

# Appendix {#appendix}

## Figures

### A. Data Visualizations

```{r}
# Display Race levels and their proportions of Arrested
proportions_table <- train_data |>
  group_by(SUSPECT_RACE_DESCRIPTION) |>
  summarise(
    Total = n(),
    Arrested = sum(SUSPECT_ARRESTED_FLAG == "Y"),
    Proportion = Arrested / Total
  ) |>
  arrange(desc(Proportion))

# Display proportion of arrested by race level as a stacked bar plot
train_data |>
  mutate(SUSPECT_RACE_DESCRIPTION = factor(
    SUSPECT_RACE_DESCRIPTION, 
    levels = proportions_table$SUSPECT_RACE_DESCRIPTION
  )) |>
  ggplot(aes(x = SUSPECT_RACE_DESCRIPTION, 
             fill = SUSPECT_ARRESTED_FLAG)) + 
  geom_bar(position = "fill") +
  labs(
    title = "Proportion Arrested",
    x = "Race Description",
    y = "Proportion"
  ) +
  theme_minimal() + 
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```


### B. Variable Importance Plots {#variable-importance-plots}

```{r}
lapply(names(models), function(model_name) {
  
  var_imp_df <- as.data.frame(varImp(models[[model_name]])$importance)
  var_imp_df$Variable <- rownames(var_imp_df)
  
  # Select the top 5 variables based on importance
  top_vars <- var_imp_df |>
    arrange(desc(Overall)) |>
    slice_head(n = 10)
  
  ggplot(top_vars, aes(x = reorder(Variable, Overall), y = Overall)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    coord_flip() +
    labs(
      title = paste("Top 10 Variables for", model_name),
      y = "Importance",
      caption = "This plot shows the top variables contributing to the model's predictive performance."
    ) +
    theme_minimal()
})

```

### C. Tuning Parameter Plots {#tuning-parameter-plots}

```{r}
lapply(names(models), function(model_name) {
  model <- models[[model_name]]
  
  if (model$bestTune[[1]] != "none") {
    plot(model, main = paste(model_name))
  }
})
```

### D. Trained model summaries  {#trained-model-summaries}

```{r}
resamples <- resamples(models)

# Customize boxplot settings for consistency
trellis.par.set(
  box.rectangle = list(fill = "steelblue", col = "black"), 
)

bwplot(resamples, metric = "ROC")
bwplot(resamples, metric = "Sens")
bwplot(resamples, metric = "Spec")
```

### E. Test Metrics {#test-metrics}

```{r}
test_metrics_df <- bind_rows(test_metrics)

# Define the "best" direction for each metric
metric_direction <- tibble(
  Metric = c("Accuracy", "AUC", "Kappa", "LogLoss"),
  Direction = c("max", "max", "max", "min")
)

# Transform data and determine "best" bars
test_metrics_df_long <- test_metrics_df |>
  pivot_longer(-Model, names_to = "Metric", values_to = "Value") |>
  left_join(metric_direction, by = "Metric") |>
  group_by(Metric) |>
  mutate(Best = ifelse(
    (Direction == "max" & Value == max(Value)) | 
    (Direction == "min" & Value == min(Value)), 
    "Best", 
    "Others"
  )) |>
  ungroup()

ggplot(test_metrics_df_long, aes(x = Model, y = Value, fill = Best)) +
  geom_bar(stat = "identity", width = 0.7) +
  facet_wrap(~ Metric, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = c("Best" = "steelblue", "Others" = "lightgrey")) +
  labs(
    title = "Model Test Performance Metrics",
    x = "Model",
    y = "Metric Value"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none", 
    strip.text = element_text(size = 12), 
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

