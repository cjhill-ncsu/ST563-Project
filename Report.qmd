---
title: "Project Report"
author:
  - name: "Kristina Golden"
  - name: "Christopher Hill"
format: 
  html:
    toc: true    
    toc-depth: 3 
editor: visual
execute:
  echo: false
  message: false
  warning: false    
editor_options: 
  chunk_output_type: console
---

\newpage

```{r}
library(tidyverse)
library(caret)
library(lattice)
library(MLmetrics)
library(knitr)

train_data <- readRDS("training_data.rds")
test_data <- readRDS("testing_data.rds")
models <- readRDS("models.rds")
```

```{r}
test_metrics <- lapply(seq_along(models), function(i) {
  model <- models[[i]]
  model_name <- names(models)[i]
  
  probs <- predict(model, newdata = test_data, type = "prob")
  preds <- predict(model, newdata = test_data)

  cm <- confusionMatrix(preds, test_data$SUSPECT_ARRESTED_FLAG)

  accuracy <- cm$overall["Accuracy"]
  kappa <- cm$overall["Kappa"]
  
  positive_class <- levels(test_data$SUSPECT_ARRESTED_FLAG)[2]
  
  log_loss <- LogLoss(
    y_pred = as.matrix(probs), 
    y_true = as.numeric(test_data$SUSPECT_ARRESTED_FLAG == positive_class)
  )
  
  auc <- AUC(
    y_pred = probs[, positive_class], 
    y_true = as.numeric(test_data$SUSPECT_ARRESTED_FLAG == positive_class)
  )

  data.frame(
    Model = model_name,
    Accuracy = accuracy,
    Kappa = kappa,
    LogLoss = log_loss,
    AUC = auc
  )
})
```


# Introduction

## Problem Description

## Study Design and Sampling Methods

## Scientific Questions to Investigate

## Overview of Conclusions

### Chosen Model

### Key Findings

## Roadmap to Follow

The [Methods](#methods) section describes the statistical models used, the steps of data preprocessing, and the procedures for hyperparameter tuning. Additionally, visual and numeric summaries of the data are discussed to set the context for the analysis. The modeling methodology is presented in detail, with a comparison of multiple models and their performance criteria.

Here, we also address the scientific questions identified in the Introduction. The results from the models are presented and interpreted, highlighting key findings and their implications.

The [Conclusions](#conclusions) section summarizes the study objectives, the findings from the analysis, and how they relate to the overall project goals. Additional observations and limitations of the analysis are also discussed.

The [Appendix](#appendix) houses tables, source code, and supplementary plots for reference.

## Data Source Reference

[NYPD Stop, Question and Frisk Data](https://www.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page)

\newpage

# Methods {#methods}

## 1. Before Modeling

### Data Preprocessing Steps

### Data Summaries

```{r}
library(dplyr)
library(knitr)

summarize_data <- function(data) {
  # Handle numeric variables
  numeric_summary <- data |>
    select(where(is.numeric)) |>
    summarise(across(everything(), ~ {
      stats <- summary(.)
      stats <- round(stats, 2)
      paste(names(stats), stats, 
            sep = "\t", collapse = "<br>") 
    })) |>
    pivot_longer(cols = everything(), 
                 names_to = "Variable", 
                 values_to = "Summary")
  
  # Handle factor variables
  factor_summary <- data |>
    select(where(is.factor)) |>
    summarise(across(everything(), ~ {
      tbl <- table(.)
      prop <- prop.table(tbl)
      # Combine counts and proportions
      paste(names(tbl), tbl, 
            sprintf("(%.2f%%)", 
                    prop * 100), 
            sep = ":  ", collapse = "<br>")
    })) |>
    pivot_longer(cols = everything(), 
                 names_to = "Variable", 
                 values_to = "Summary")
  
  # Combine and display
  bind_rows(numeric_summary, 
            factor_summary) |>
    select(Variable, Summary) |>
    kable(format = "html", 
          col.names = c("Variable", "Summary"), 
          escape = FALSE)
}

summarize_data(train_data)
```



## 2. Statistical Models and Methods

We follow the guidelines for statistical modeling laid out in the Model Building and Data Splitting lecture notes.

1. *Split the data into a training set and a test set.*

2. *Tune hyperparameters (of all the models under consideration) using the training set:*
   a. *Split the training set further into two sets: one for fitting the model (a new training set), and the other for evaluating model performance (known as validation set or holdout set).*
   b. *For each candidate value of hyperparameter(s), fit the model using the new training set, and evaluate the fitted model using the validation set using a metric of our choice.*
   c. *Typically, we repeat steps a. and b. a few times so that we get repeated measurements of model performance for each value of hyperparameters. The final model performance is taken to be the average of these multiple measurements.*
   d. *Choose the best value of hyperparameters by optimizing the model performance measure obtained in step c.*

3. *Using the best value of hyperparameters, fit the model(s) on the entire training set and estimate the model parameters. This is (are) the final model(s) chosen using the training set.*

4. *Use the test set to estimate the model performance of the final model(s) from step 3.*

5. *Again, we may want to repeat steps 1â€“4 a few times to get a reliable estimate of model performance of the final models. For example, we can use cross-validation here to incorporate the uncertainty due to hyperparameter tuning as well.*

By leveraging the `caret` package in R, we can efficiently implement the above steps for a variety of models. The `train()` function in `caret` will handle steps 2 and 3 for us. This can be termed the "inner loop". After tuning hyperparameters and fitting the final trained model, we can then evaluate the various models on the test set. This can be termed the "outer loop". 

To evaluate the models in the outer loop and the very first step when modeling began, we used a 70/30 split, where 70% of the data was used for training and 30% for testing. This is known as the holdout method. For training in the inner loop, 5-fold cross validation was used on the training data split. The test data was never used in the inner loop, only for evaluating the final model performance.

### Model Descriptions and Assumptions

- **Random Forest** - Improves prediction accuracy by combining multiple decision trees. Each tree is trained on a random bootstrap sample of the data, and at every split, only a random subset of predictors is considered. This medigates the correlation between trees when a certain predictor may be present in all trees, reducing their variance and improving the robustness of the predictions.

- **Extreme Gradient Boosting (XGBoost)** - Builds models sequentially, each new tree correcting errors made by previous ones, and optimizes a loss function. XGBoost also uses regularization, meaning it penalizes complex models to prevent overfitting.

- **Logistic Regression** - Models the relationship between a set of predictor variables and a binary outcome by estimating the probability of the outcome occurring, using the logistic function to ensure probabilities fall between 0 and 1. The method identifies linear relationships between predictors and the log-odds of the outcome

- **Elastic Net** - A regularized regression method that combines the penalties of Lasso and Ridge regression. It balances variable selection and shrinkage by minimizing prediction error while preventing overfitting.

### Model Comparisons

#### Variables of Importance

[Variables of Importance](#variable-importance-plots)

#### Tuning Parameters

[Hyperparameter Tuning](#tuning-parameter-plots)

#### Trained Model Summaries

[Trained Model Summaries](#trained-model-summaries)

### Scientific Questions and Results Interpretation

## Performance Investigation and Criteria

We evaluate several metrics to compare the performance of the models as they generilize to our test set. These metrics include: AUC, LogLoss, Kappa, and Accuracy. 

[Test Metrics Results](#test-metrics)

### Metrics Used for Model Evaluation

- **AUC** (Area Under the *Receiver Operating Characteristic* Curve) - was used as our training metric of choice during model tuning. AUC measures how correctly a model distinguishes between true outcomes and false outcomes across all possible boundaries between classes. A model with a larger AUC value can be said to better distinguish between true and false outcomes across all boundaries between classes.

- **LogLoss** - quantifies how uncertain a model is about its predictions, penalizing incorrect predictions with greater uncertainty. A model with a smaller LogLoss value can be said to make more confident and accurate predictions because it assigns higher probabilities to correct outcomes and lower probabilities to incorrect ones, minimizing uncertainty.

- **Kappa** - measures how correct a model's predictions are compared to actual outcomes, while accounting for correctness expected by random chance. A model with a larger Kappa value can be said to make predictions that are more correct relative to chance alone.

- **Accuracy** - is the proportion of correctly predicted outcomes out of all predictions made. Included for its simplicity and interpretability.

## Limitations of Methods

\newpage

# Conclusions {#conclusions}

## Study Objectives Summary

## Findings and Interpretation

## Project Goals and Outcomes

## Additional Observations

\newpage

# Appendix {#appendix}

## Figures

### A. Data Visualizations

```{r}
# Display Race levels and their proportions of Arrested
proportions_table <- train_data |>
  group_by(SUSPECT_RACE_DESCRIPTION) |>
  summarise(
    Total = n(),
    Arrested = sum(SUSPECT_ARRESTED_FLAG == "Y"),
    Proportion = Arrested / Total
  ) |>
  arrange(desc(Proportion))

# Display proportion of arrested by race level as a stacked bar plot
train_data |>
  mutate(SUSPECT_RACE_DESCRIPTION = factor(
    SUSPECT_RACE_DESCRIPTION, 
    levels = proportions_table$SUSPECT_RACE_DESCRIPTION
  )) |>
  ggplot(aes(x = SUSPECT_RACE_DESCRIPTION, 
             fill = SUSPECT_ARRESTED_FLAG)) + 
  geom_bar(position = "fill") +
  labs(
    title = "Proportion Arrested",
    x = "Race Description",
    y = "Proportion"
  ) +
  theme_minimal() + 
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```


### B. Variable Importance Plots {#variable-importance-plots}

```{r}
lapply(names(models), function(model_name) {
  
  var_imp_df <- as.data.frame(varImp(models[[model_name]])$importance)
  var_imp_df$Variable <- rownames(var_imp_df)
  
  # Select the top 5 variables based on importance
  top_vars <- var_imp_df |>
    arrange(desc(Overall)) |>
    slice_head(n = 10)
  
  ggplot(top_vars, aes(x = reorder(Variable, Overall), y = Overall)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    coord_flip() +
    labs(
      title = paste("Top 10 Variables for", model_name),
      y = "Importance",
      caption = "This plot shows the top variables contributing to the model's predictive performance."
    ) +
    theme_minimal()
})

```

### C. Tuning Parameter Plots {#tuning-parameter-plots}

```{r}
lapply(names(models), function(model_name) {
  model <- models[[model_name]]
  
  if (model$bestTune[[1]] != "none") {
    plot(model, main = paste(model_name))
  }
})
```

### D. Trained model summaries  {#trained-model-summaries}

```{r}
resamples <- resamples(models)

# Customize boxplot settings for consistency
trellis.par.set(
  box.rectangle = list(fill = "steelblue", col = "black"), 
)

bwplot(resamples, metric = "ROC")
bwplot(resamples, metric = "Sens")
bwplot(resamples, metric = "Spec")
```

### E. Test Metrics {#test-metrics}

```{r}
test_metrics_df <- bind_rows(test_metrics)

# Define the "best" direction for each metric
metric_direction <- tibble(
  Metric = c("Accuracy", "AUC", "Kappa", "LogLoss"),
  Direction = c("max", "max", "max", "min")
)

# Transform data and determine "best" bars
test_metrics_df_long <- test_metrics_df |>
  pivot_longer(-Model, names_to = "Metric", values_to = "Value") |>
  left_join(metric_direction, by = "Metric") |>
  group_by(Metric) |>
  mutate(Best = ifelse(
    (Direction == "max" & Value == max(Value)) | 
    (Direction == "min" & Value == min(Value)), 
    "Best", 
    "Others"
  )) |>
  ungroup()

ggplot(test_metrics_df_long, aes(x = Model, y = Value, fill = Best)) +
  geom_bar(stat = "identity", width = 0.7) +
  facet_wrap(~ Metric, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = c("Best" = "steelblue", "Others" = "lightgrey")) +
  labs(
    title = "Model Test Performance Metrics",
    x = "Model",
    y = "Metric Value"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none", 
    strip.text = element_text(size = 12), 
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

